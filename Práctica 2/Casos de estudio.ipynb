{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95e3d89-7740-4f1f-97c8-b2e35232619b",
   "metadata": {},
   "source": [
    "# Importando los paquetes que vamos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed4aee-0431-4cbd-a176-4a0e558784e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Algoritmos de clusterizacion\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Para reducir dimensionalidad\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Metricas de los clusters\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Para la anotacion de tipos\n",
    "from typing import Callable, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc3d1f-df97-40b8-9724-c9af7ed7546a",
   "metadata": {},
   "source": [
    "# Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da44ec-e6dd-440b-862c-bc8ef2cb4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"dataset/datos_hogar_2020.csv\"\n",
    "df_global = pd.read_csv(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95387038-17b3-457d-9cbc-9a18b75d46f7",
   "metadata": {},
   "source": [
    "# Funciones comunes a todos los casos de estudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a67181-ce89-4423-8d7f-253d7ed71596",
   "metadata": {},
   "source": [
    "Definimos en esta sección las funciones que vamos a usar a lo largo de todos los casos de estudio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39475b82-1d23-492f-a0a0-35687ab7ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df: pd.DataFrame, exclude_columns: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Dada una lista con columnas a excluir, devuelve un nuevo dataframe en el que \n",
    "    hemos quitado estas columnas. Si exclude_columns es None, no se hace nada.\n",
    "    \n",
    "    En ningun caso se modifica df, se hace una copia de este y se opera con \n",
    "    dicha copia\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hacemos una copia del dataframe\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Excluimos las columnas en caso de que se haya pasado una lista no None\n",
    "    if exclude_columns is not None:\n",
    "        df_filtered = df_filtered.drop(columns = exclude_columns)\n",
    "    \n",
    "    # Devolvemos el dataframe copia filtrado\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d3036-0d2a-42b9-8968-202f72d6c61d",
   "metadata": {},
   "source": [
    "## Decorador para tomar el tiempo de ejecución de una función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6527475-844b-4407-92f0-ca3888625f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_func(func):\n",
    "    \"\"\"\n",
    "    Decorador que toma una funcion func -> result y devuelve una funcion \n",
    "    func' -> result, ellapsed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Definimos la funcion modificada como hemos especificado en la documentacion\n",
    "    def modified_function(*args, **kwargs):\n",
    "        init = time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time()\n",
    "        \n",
    "        # Devolvemos el resultado junto con el tiempo que tardo en ejecutarse\n",
    "        return result, end - init\n",
    "    \n",
    "    return modified_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715118f-1283-4361-afd9-4abe7087c556",
   "metadata": {},
   "source": [
    "## Funciones para calcular clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa95f9-42de-4564-95b3-6251c6c0bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_func\n",
    "def compute_kmeans(df: pd.DataFrame, number_clusters: int = 3) -> np.array:\n",
    "    \"\"\"\n",
    "    Dado un dataframe con los datos a clusterizar, calcula el vector con las\n",
    "    etiquetas de la clusterizacion kmeans\n",
    "    \"\"\"\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = number_clusters, random_state=0).fit(df)\n",
    "    return kmeans.labels_\n",
    "\n",
    "@time_func\n",
    "def compute_dbscan(df: pd.DataFrame, eps: float = 0.3, min_samples: int = 10) -> np.array:\n",
    "    \"\"\"\n",
    "    Dado un dataframe con los datos a clusterizar, calcula el vector con las\n",
    "    etiquetas de la clusterizacion dbscan\n",
    "    \"\"\"\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs = 2).fit(df)\n",
    "    return db.labels_\n",
    "\n",
    "@time_func\n",
    "def compute_birch(df: pd.DataFrame, num_clusters: int = None, threshold: float = 0.5) -> np.array:\n",
    "    \"\"\"\n",
    "    Dado un dataframe con los datos a clusterizar, calcula el vector con las\n",
    "    etiquetas de la clusterizacion birch\n",
    "    \"\"\"\n",
    "    \n",
    "    brc = Birch(n_clusters=num_clusters, threshold = threshold)\n",
    "    brc.fit(df)\n",
    "    return brc.labels_\n",
    "\n",
    "@time_func\n",
    "def compute_mean_shift(df: pd.DataFrame, bandwidth: float = 2.0, max_iter: int = 300) -> np.array:\n",
    "    \"\"\"\n",
    "    Dado un dataframe con los datos a clusterizar, calcula el vector con las\n",
    "    etiquetas de la clusterizacion Mean Shift\n",
    "    \"\"\"\n",
    "\n",
    "    clustering = MeanShift(bandwidth=bandwidth, max_iter = max_iter).fit(df)\n",
    "    return clustering.labels_\n",
    "\n",
    "@time_func\n",
    "def compute_agglomerative_clustering(df: pd.DataFrame, n_clusters: int = None, linkage: str = \"ward\") -> np.array:\n",
    "    \"\"\"\n",
    "    Dado un dataframe con los datos a clusterizar, calcula el vector con las\n",
    "    etiquetas de la clusterizacion de forma jerarquica\n",
    "    \"\"\"\n",
    "    \n",
    "    clustering = AgglomerativeClustering(n_clusters = n_clusters, linkage = linkage).fit(df)\n",
    "    return clustering.labels_\n",
    "\n",
    "def add_clustering_labels(df: pd.DataFrame, clustering_function: Callable, parameters: dict(), new_label_name: str, exclude_columns: List[str] = None) -> Tuple[pd.DataFrame, float]:\n",
    "    \"\"\"\n",
    "    Dado un dataframe y un algoritmo de clusterizacion con sus parametros, devuelve el dataframe al que\n",
    "    hemos añadido una columna con las nuevas etiquetas del clustering. Con esto nos evitamos escribir\n",
    "    esta funcion para cada tipo de algorito para clusterizar. Solo tenemos que cambiar ese parametro\n",
    "    \n",
    "    Podemos excluir columnas para que no se usen en la clusterizacion\n",
    "    \n",
    "    Devuelve un dataframe con la nueva columna de etiquetas añadida y con el tiempo que tardo en ejecutarse\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtramos las columnas del dataframe que se quieren exluir del computo\n",
    "    df_filtered = drop_columns(df, exclude_columns)\n",
    "    \n",
    "    # Realizamos el calculo\n",
    "    # Todas las funciones que calculan clusters llevan el decorador @time_func \n",
    "    # y por tanto devuelven cuanto tiempo an tardado \n",
    "    labels, ellapsed = clustering_function(df_filtered, **parameters)\n",
    "\n",
    "    # Añadimos las etiquetas a la vez que recuperamos las columnas eliminadas\n",
    "    new_df = df.copy()\n",
    "    new_df[new_label_name] = labels\n",
    "    \n",
    "    return new_df, ellapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfec9a-1c88-4e12-a0a6-934574ab47a1",
   "metadata": {},
   "source": [
    "## Funciones para evaluar clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b8ec7-228f-4881-b211-38a51b31a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_silhouette(df: pd.DataFrame, labels: np.array, exclude_columns: List[str] = None) -> float:\n",
    "    \"\"\"Calcula el indice de silhouette de una clusterizacion\"\"\"\n",
    "    \n",
    "    # Excluimos las columnas si se ha especificado asi por parametro\n",
    "    df_filtered = drop_columns(df, exclude_columns)\n",
    "    \n",
    "    # Devolvemos la metrica buscada\n",
    "    return metrics.silhouette_score(df_filtered, labels, metric='euclidean')\n",
    "\n",
    "def metric_calinski(df: pd.DataFrame, labels: np.array, exclude_columns: List[str] = None) -> float:\n",
    "    \"\"\"Calcula el indice de Calinski-Harabasz de una clusterizacion\"\"\"\n",
    "    \n",
    "    # Excluimos las columnas si se ha especificado asi por parametro\n",
    "    df_filtered = drop_columns(df, exclude_columns)\n",
    "    \n",
    "    # Devolvemos la metrica buscada\n",
    "    return metrics.calinski_harabasz_score(df_filtered, labels)\n",
    "\n",
    "def metric_davies_bouldin(df: pd.DataFrame, labels: np.array, exclude_columns: List[str] = None) -> float:\n",
    "    \"\"\"Calcula el indice de Davies-Bouldin de una clusterizacion\"\"\"\n",
    "    \n",
    "    # Excluimos las columnas si se ha especificado asi por parametro\n",
    "    df_filtered = drop_columns(df, exclude_columns)\n",
    "    \n",
    "    # Devolvemos la metrica buscada\n",
    "    return metrics.davies_bouldin_score(df_filtered, labels)\n",
    "\n",
    "def metric_elements_in_each_cluster(labels: np.array) -> Tuple[float, float]:\n",
    "    \"\"\"Calcula el numero medio de elementos por cada cluster, y la desviacion tipica del\n",
    "    numero de elementos por cada cluster\"\"\"\n",
    "    \n",
    "    # Tomamos los valores unicos en el conjunto de etiquetas\n",
    "    # Esto nos da las etiquetas de los clusters que hemos construido\n",
    "    cluster_labels = set(labels)\n",
    "    \n",
    "    # Calculamos el numero de elementos que hay en cada cluster\n",
    "    elements_in_each_cluster = []\n",
    "    \n",
    "    for label in cluster_labels:\n",
    "        curr_labels = [el for el in labels if el == label]\n",
    "        elements_in_each_cluster.append(len(curr_labels))\n",
    "    \n",
    "    \n",
    "    # Calculamos las estadisticas usando numpy\n",
    "    elements_in_each_cluster = np.array(elements_in_each_cluster)\n",
    "    mean = np.mean(elements_in_each_cluster)\n",
    "    std = np.std(elements_in_each_cluster)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def metric_total_clusters(labels: np.array) -> int:\n",
    "    \"\"\"Calcula el numero total de clusters que hemos generado con un algoritmo\"\"\"\n",
    "    \n",
    "    # Tomamos los valores unicos en el conjunto de etiquetas\n",
    "    # Esto nos da las etiquetas de los clusters que hemos construido\n",
    "    cluster_labels = set(labels)\n",
    "    \n",
    "    # Con esto, solo tenemos que devolver el tamaño de dicho conjunto\n",
    "    return len(cluster_labels)\n",
    "    \n",
    "def compute_clustering_metrics(df: pd.DataFrame, labels: np.array) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computa todas las metricas que tenemos sobre un clustering.\n",
    "    \n",
    "    @param df el dataframe, con solo los datos de entrada (quitar etiquetas y otros metadatos)\n",
    "    @param labels las etiquetas obtenidas en un proceso de clustering\n",
    "    \n",
    "    @return metrics diccionario con pares \"nombre_metrica\": valor_metrica \n",
    "    \"\"\"\n",
    "               \n",
    "    # Aplicamos todos los computos necesarios\n",
    "    silhouette = metric_silhouette(df, labels)\n",
    "    calinski = metric_calinski(df, labels)\n",
    "    davies = metric_davies_bouldin(df, labels)\n",
    "    total_clusters = metric_total_clusters(labels)\n",
    "    elements_mean, elements_std = metric_elements_in_each_cluster(labels)\n",
    "    \n",
    "    # Devolvemos todas las metricas en un diccionario\n",
    "    metrics = dict()\n",
    "    metrics[\"silhouette\"] = silhouette\n",
    "    metrics[\"calinski\"] = calinski\n",
    "    metrics[\"davies\"] = davies\n",
    "    metrics[\"total_clusters\"] = total_clusters\n",
    "    metrics[\"elements_mean\"] = elements_mean\n",
    "    metrics[\"elements_std\"] = elements_std\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba708a-9b9d-4023-8561-50ff77afe2f1",
   "metadata": {},
   "source": [
    "## Funciones para realizar visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebc47f-bcde-4e5a-adb5-43a3b6d8c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pair_plot(df: pd.DataFrame, hue: str = None, exclude_columns: List[str] = None):\n",
    "    \"\"\"\n",
    "    Muestra una grafica pairplot de seaborn. Podemos especificar una lista de\n",
    "    columnas que exluir de esta grafica de forma comoda\n",
    "    \"\"\"\n",
    "    df_filtered = df.copy()\n",
    "    if exclude_columns is not None:\n",
    "        df_filtered = df_filtered.drop(columns = exclude_columns)\n",
    "        \n",
    "    sns.pairplot(df_filtered, hue = hue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a23258-55ca-41e1-82c4-5649e460a989",
   "metadata": {},
   "source": [
    "## Funciones para normalizar dataframes y borrar outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92684629-992c-4c57-aaed-87af945649d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalizamos las columnas de un dataframe para que tengan media\n",
    "    0 y desviacion tipica 1\"\"\"\n",
    "    \n",
    "    df_norm = df.copy()\n",
    "    df_norm = (df_norm - df_norm.mean()) / df_norm.std()\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "def normalize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalizamos las columnas de un dataframe al rango [0, 1]\"\"\"\n",
    "    \n",
    "    df_norm = df.copy()\n",
    "    df_norm = (df_norm - df_norm.min()) / (df_norm.max() - df_norm.min())\n",
    "    \n",
    "    return df_norm\n",
    "\n",
    "def remove_outliers(df: pd.DataFrame, iqr_threshold: float = 3.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Borra outliers quitando aquellas fila en las que tengamos un outlier en alguna columna\n",
    "    Consideramos Outliers aquellos que se alejan de la media más de iqr_threshold veces la \n",
    "    desviacion tipica (normalmente se usa un valor 3.0)\n",
    "    \n",
    "    Se devuelve un dataframe nuevo, el original no se modifica\n",
    "    \"\"\"\n",
    "    \n",
    "    df_filtered = df.copy()\n",
    "    return df_filtered[(np.abs(stats.zscore(df_filtered)) < 3).all(axis=1)]\n",
    "\n",
    "def nan_to_zero(value):\n",
    "    \"\"\"Hace cero el valor si es None o NaN. En otro caso devuelve el valor\"\"\"\n",
    "    \n",
    "    if value is None or math.isnan(value):\n",
    "        return 0.0\n",
    "    \n",
    "    # El valor no tiene que ser tratado, asi que se devuelve sin modificar\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba84385-a7a9-4d90-97c2-bfb283599c76",
   "metadata": {},
   "source": [
    "# Filtrado previo del dataset global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3c924-0337-4dba-b464-ab4c3ae0ddf3",
   "metadata": {},
   "source": [
    "Definimos en variables todas las columnas que, al leer el documento explicando el *dataset*, nos han resultado de interés. Una vez hecho esto, filtramos el *dataset* global para quedarnos solo con esas columnas, pues el resto no las vamos a usar en ningún caso de estudio. Es más, muchas de estas columnas no las usaremos en ninguno de los tres casos de estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6d05a-daff-423b-96a3-2d52e2a0c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las columnas con las que nos vamos a quedar\n",
    "renta_disponible_total_hogar = \"HY020\"\n",
    "renta_ganada_alquilando = \"HY040N\"\n",
    "ayuda_para_vivienda = \"HY070N\"\n",
    "inversiones_de_capital = \"HY090N\"\n",
    "retraso_pago_hipoteca_o_alquiler = \"HS011\"\n",
    "retraso_pago_luz_agua_gas = \"HS021\"\n",
    "retraso_pago_prestamos = \"HS031\"\n",
    "capacidad_hogar_llegar_fin_mes = \"HS120\"\n",
    "ingresos_minimos_para_llegar_fin_mes = \"HS130\"\n",
    "vandalismo_en_la_zona = \"HS190\"\n",
    "comunidad_autonoma = \"DB040\"\n",
    "tipo_de_vivienda = \"HH010\"\n",
    "regimen_de_tenencia_vivienda = \"HH021\"\n",
    "habitaciones_vivienda = \"HH030\"\n",
    "pidio_ayuda_economica_familiares = \"H39A_U\"\n",
    "pidio_ayuda_economica_entidad = \"H39B_U\"\n",
    "corte_energia = \"H38A_U\"\n",
    "numero_miembros_hogar = \"HX040\"\n",
    "tipo_de_hogar = \"HX060\" # Variable numerica categorica con muchos valores posibles\n",
    "hogar_en_riesgo_pobreza = \"vhPobreza\"\n",
    "hogar_carencia_material_severa = \"vhMATDEP\"\n",
    "proposito_prestamos = lambda i: f\"HI100_{i}\"\n",
    "cuota_hipoteca = \"cuotahip\"\n",
    "gasto_en_comida = \"HC010\"\n",
    "gasto_en_comida_bebida_fuera_casa = \"HC020\"\n",
    "gasto_transporte_publico = \"HC030\"\n",
    "gasto_transporte_privado = \"HC040\"\n",
    "gastos_vivienda_mensual = \"HH070\"\n",
    "viviendas_adicionales = \"HV020\"\n",
    "renta_disponible_restado_alquiler = \"vhRentaAIa\"\n",
    "\n",
    "# Filtramos todo el dataframe global para usar solo variables que en una primera\n",
    "# lectura de las variables hemos considerado interesantes\n",
    "df_global = df_global[[\n",
    "    renta_disponible_total_hogar,\n",
    "    renta_ganada_alquilando,\n",
    "    ayuda_para_vivienda,\n",
    "    inversiones_de_capital,\n",
    "    retraso_pago_hipoteca_o_alquiler,\n",
    "    retraso_pago_luz_agua_gas,\n",
    "    retraso_pago_prestamos,\n",
    "    capacidad_hogar_llegar_fin_mes,\n",
    "    ingresos_minimos_para_llegar_fin_mes,\n",
    "    vandalismo_en_la_zona,\n",
    "    tipo_de_vivienda,\n",
    "    regimen_de_tenencia_vivienda,\n",
    "    habitaciones_vivienda,   \n",
    "    pidio_ayuda_economica_familiares,\n",
    "    pidio_ayuda_economica_entidad,\n",
    "    corte_energia,\n",
    "    numero_miembros_hogar,\n",
    "    tipo_de_hogar,\n",
    "    hogar_en_riesgo_pobreza,\n",
    "    hogar_carencia_material_severa,\n",
    "    comunidad_autonoma,\n",
    "    \n",
    "    gasto_en_comida,\n",
    "    gasto_en_comida_bebida_fuera_casa,\n",
    "    gasto_transporte_publico,\n",
    "    gasto_transporte_privado,\n",
    "    gastos_vivienda_mensual,\n",
    "    viviendas_adicionales,\n",
    "\n",
    "    # TODO -- borrar los propositos porque no parecen muy relevantes\n",
    "    proposito_prestamos(1),\n",
    "    proposito_prestamos(2),\n",
    "    proposito_prestamos(3),\n",
    "    proposito_prestamos(4),\n",
    "    proposito_prestamos(5),\n",
    "    proposito_prestamos(6),\n",
    "    proposito_prestamos(7),\n",
    "    proposito_prestamos(8),\n",
    "    proposito_prestamos(9),  \n",
    "    \n",
    "    cuota_hipoteca,\n",
    "    renta_disponible_restado_alquiler,\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3a8ee-b15b-4d78-b5a5-1f39efa6f698",
   "metadata": {},
   "source": [
    "Veamos con qué tipo de datos nos hemos quedado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5adfa-469e-4a51-bcad-e71578236f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330a603-a320-484d-a079-74f8f4a22746",
   "metadata": {},
   "source": [
    "# Variables añadidas\n",
    "\n",
    "También queremos usar variables agregadas usando las variables en bruto que ya tenemos en el dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeafc14-27bf-4e81-9f37-fbe862fb7335",
   "metadata": {},
   "source": [
    "Tenemos tres tipos de variable que nos indican si pidieron ayuda a algún ente, divididas en tres tipos de ente. Nos interesa solo si pidieron ayuda, no a quién"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f42a49-f8b9-4978-be4a-c4150fb2f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pidio_ayuda_condicion(ayuda_familiar, ayuda_entidad):\n",
    "    \"\"\"Comprueba si, dadas las columnas de ayuda familiar y ayuda de entidad,\n",
    "    se pidio ayuda o no se pidio\"\"\"\n",
    "    \n",
    "    if ayuda_familiar == 1 or ayuda_entidad == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# Añadimos la nueva columna usando la condicion que hemos definido previamente\n",
    "pidio_ayuda = \"pidio_ayuda\"\n",
    "df_global[pidio_ayuda] = df_global.apply(lambda row: pidio_ayuda_condicion(row[pidio_ayuda_economica_familiares], row[pidio_ayuda_economica_entidad]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a9a05-2d06-498d-b9d1-f94429d99af3",
   "metadata": {},
   "source": [
    "Añadimos una variable agregando todos los gastos en transporte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cbd05-e42e-42c9-9cfc-1f03a903fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_gastos(publico, privado):\n",
    "    \n",
    "    # Tratamos los valores None o NaN\n",
    "    # Para poder sumar ambos valores\n",
    "    publico = nan_to_zero(publico)\n",
    "    privado = nan_to_zero(privado)\n",
    "        \n",
    "    return publico + privado\n",
    "\n",
    "gasto_transporte_total = \"gasto_transporte_total\"\n",
    "df_global[gasto_transporte_total] = df_global.apply(lambda row: sum_gastos(row[gasto_transporte_privado], row[gasto_transporte_publico]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a3094-c7d2-4da9-a16c-b5a8e9a3bca7",
   "metadata": {},
   "source": [
    "Añadimos una variable que recoja si ha habido retraso en cualquier tipo de pago:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2211a1-39cc-4f46-a859-7e8abd285e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retraso_pagando(retraso_hipoteca, retraso_luz_agua, retraso_prestamos):\n",
    "    \"\"\"Comprueba si hubo algun retraso en cualquier tipo de pago\"\"\"\n",
    "    \n",
    "    # Hubo retraso en la hipoteca\n",
    "    if retraso_hipoteca is not None and retraso_hipoteca != 3:\n",
    "        return 1\n",
    "    \n",
    "    # Hubo retraso en la luz o el agua\n",
    "    if retraso_luz_agua is not None and retraso_luz_agua != 3:\n",
    "        return 1\n",
    "    \n",
    "    # Hubo retraso en algun prestamo\n",
    "    if retraso_prestamos is not None and retraso_prestamos != 3:\n",
    "        return 1\n",
    "\n",
    "    # No hubo retraso en ningun apartado\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Añadimos la nueva columna usando la condicion que hemos definido previamente\n",
    "retraso_pago = \"retraso_pago\"\n",
    "df_global[retraso_pago] = df_global.apply(lambda row: retraso_pagando(row[retraso_pago_hipoteca_o_alquiler], row[retraso_pago_luz_agua_gas], row[retraso_pago_prestamos]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d59fca-351a-45ee-919e-b55bc6669306",
   "metadata": {},
   "source": [
    "Añadimos una variable en la que recojamos los ingresos menos los gastos totales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38c085-f3d6-43d0-9d7c-bf774d6b7289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_ingresos_menos_gastos(renta_disponible_total_hogar, gasto_en_comida, gasto_en_comida_bebida_fuera_casa, gastos_vivienda_mensual, gasto_transporte_total):\n",
    "    \"\"\"Calcula la resta de todos los ingresos - todos los gastos\"\"\"\n",
    "    \n",
    "    # Hacemos 0.0 los valores None o NaN para poder realizar operaciones aritmeticas\n",
    "    renta_disponible_total_hogar_val = nan_to_zero(renta_disponible_total_hogar)\n",
    "    gasto_en_comida_val = nan_to_zero(gasto_en_comida)\n",
    "    gasto_en_comida_bebida_fuera_casa_val = nan_to_zero(gasto_en_comida_bebida_fuera_casa)\n",
    "    gastos_vivienda_mensual_val = nan_to_zero(gastos_vivienda_mensual)\n",
    "    gasto_transporte_total_val = nan_to_zero(gasto_transporte_total)\n",
    "    \n",
    "    return renta_disponible_total_hogar_val - gasto_en_comida_val - gasto_en_comida_bebida_fuera_casa_val - gastos_vivienda_mensual_val - gasto_transporte_total_val\n",
    "\n",
    "\n",
    "ingresos_menos_gastos = \"ingresos_menos_gastos\"\n",
    "df_global[ingresos_menos_gastos] = df_global.apply(\n",
    "    lambda row: calcular_ingresos_menos_gastos(row[renta_disponible_total_hogar], row[gasto_en_comida], row[gasto_en_comida_bebida_fuera_casa], row[gastos_vivienda_mensual], row[gasto_transporte_total]), \n",
    "    axis = 1)\n",
    "df_global[ingresos_menos_gastos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6ac45c-7f60-44e6-b846-a31305ee9672",
   "metadata": {},
   "source": [
    "Añadimos el nombre real de las comunidades autonomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd1a8b-7f28-4d79-8f06-41ef67a7c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_nombre_comunidad(code_name):\n",
    "    \"\"\"Transformamos el codigo de la comunidad autonoma a su nombre real\"\"\"\n",
    "    map_code_to_name = {\n",
    "        \"ES11\": \"Galicia\", \n",
    "        \"ES12\": \"Principado de Asturias\" , \n",
    "        \"ES13\": \"Cantabria\" , \n",
    "        \"ES21\": \"País Vasco\" , \n",
    "        \"ES22\": \"Comunidad Foral de Navarra\" , \n",
    "        \"ES23\": \"La Rioja\" , \n",
    "        \"ES24\": \"Aragón\" , \n",
    "        \"ES30\": \"Madrid\" , \n",
    "        \"ES41\": \"Castilla y Leon\" , \n",
    "        \"ES42\": \"Castilla-La Mancha\" , \n",
    "        \"ES43\": \"Extremadura\", \n",
    "        \"ES51\": \"Cataluña\" , \n",
    "        \"ES52\": \"Comunidad Valenciana\" , \n",
    "        \"ES53\": \"Illes Balears\" , \n",
    "        \"ES61\": \"Andalucía\" , \n",
    "        \"ES62\": \"Región de Murcia\" , \n",
    "        \"ES63\": \"Ceuta\" , \n",
    "        \"ES64\": \"Melilla\" , \n",
    "        \"ES70\": \"Canarias\" , \n",
    "        \"ESZZ\": \"Externo\" , \n",
    "    }\n",
    "    \n",
    "    return map_code_to_name[code_name]\n",
    "\n",
    "df_global[comunidad_autonoma] = df_global.apply(lambda row: map_nombre_comunidad(row[comunidad_autonoma]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fedc4d-f774-4fce-a571-95b512f4df6c",
   "metadata": {},
   "source": [
    "# Caso de estudio 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb701297-a55a-4bf8-a8f1-35701ebef77d",
   "metadata": {},
   "source": [
    "## Definición del caso de estudio\n",
    "\n",
    "En este caso de estudio queremos poner nuestra atención en aquellas personas que tuvieron algún retraso en cualquier tipo de pago. Usamos la variable que hemos definido previamente como agregado de los dos tipos de *retraso en el pago*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf198ee-1af6-4db0-a101-58c8d7204b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos los datos de aquellos hogares que pidieron ayuda economica\n",
    "df_first = df_global[df_global[retraso_pago] == 1]\n",
    "\n",
    "# Nos quedamos con las columnas que nos interesan para este caso de estudio\n",
    "columnas_interesantes = [\n",
    "    renta_disponible_total_hogar,\n",
    "    ingresos_minimos_para_llegar_fin_mes,\n",
    "    gasto_en_comida,\n",
    "    gastos_vivienda_mensual,\n",
    "    gasto_transporte_total,\n",
    "]\n",
    "df_first = df_first[columnas_interesantes]\n",
    "\n",
    "# Renombramos algunas columnas\n",
    "df_first = df_first.rename(columns = {\n",
    "    renta_disponible_total_hogar: \"renta_disponible_hogar\",\n",
    "    ingresos_minimos_para_llegar_fin_mes: \"ingresos_minimos_para_llegar_fin_mes\",\n",
    "    gasto_en_comida: \"gasto_en_comida\",\n",
    "    gastos_vivienda_mensual: \"gastos_vivienda_mensual\",\n",
    "    gasto_transporte_total: \"gasto_transporte_total\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9f232-3f2e-440f-8393-11d90eba44aa",
   "metadata": {},
   "source": [
    "## Preprocesado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e4aa2-60ce-4075-8b78-3852b1834a84",
   "metadata": {},
   "source": [
    "Ahora que hemos seleccionado las variables, borramos aquellas filas en las que tengamos algún valor NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5721ed-74c6-432b-a16b-e130ddf91f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Antes: {df_first.shape}\")\n",
    "df_first = df_first.dropna()\n",
    "print(f\"Despues: {df_first.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9794154-7494-48dd-9cf3-a35ecaac7b48",
   "metadata": {},
   "source": [
    "Borramos los *outliers* usando $3 \\cdot IQR$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fc3c2-72bc-4fc4-82a8-36ff523e3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Antes: {df_first.shape}\")\n",
    "df_first = remove_outliers(df_first, 3.0)\n",
    "print(f\"Despues: {df_first.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f67a5-290a-4877-ac7c-271cc9af7030",
   "metadata": {},
   "source": [
    "Normalizamos todos los datos, pues tener variables en distintas escalas afecta enormemente a los algoritmos de clusterización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44c6c8-e362-45e7-88d1-9dd3f7a3e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = normalize_dataframe(df_first)\n",
    "df_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b7b04-6a2b-41e7-a66e-d3e429a92462",
   "metadata": {},
   "source": [
    "## Análisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3976ee-c536-414e-8e92-2e45f2faa632",
   "metadata": {},
   "source": [
    "Antes de comenzar con el cálculo de *clusters*, realizamos un análisis exploratorio de datos previo. Empezamos mostrando las gráficas entre las variables con las que trabajamos, dos a dos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b6a97-d5a2-47a4-a416-20fe97819724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1b97d-d275-4f9c-9afe-5c5ddc31990a",
   "metadata": {},
   "source": [
    "En un primer momento, consideramos usar las variables `renta_disponible_total_hogar` y `renta_disponible_restado_alquiler`. Sin embargo, como mostramos a continuación, están muy correladas, por lo que no aportan información interesante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718c656-a04f-4ed1-963a-8366a71c85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_global[[renta_disponible_total_hogar, renta_disponible_restado_alquiler]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b9225-43b3-4004-bbb5-cd41c8a66cd2",
   "metadata": {},
   "source": [
    "También calculábamos el valor de $total\\_ingresos - total\\_gastos$, pero vemos una gran correlación, como mostramos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4234bf4-484d-4170-bbf2-3e3ec2d28593",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_global[[renta_disponible_total_hogar, ingresos_menos_gastos]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb456ce-ba30-402b-aed3-d52774e13361",
   "metadata": {},
   "source": [
    "## Aplicamos algoritmos de clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18624a7e-1f58-4d82-b5eb-bdb457846a2d",
   "metadata": {},
   "source": [
    "En todos los algoritmos estamos calculando los tiempos que tardan en ejecutarse. Esto lo guardaremos en el siguiente diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543a6e3-bf6c-4704-962f-097f677475c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ellapsed_times = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a048230-253d-4557-b2f9-b99ff4851188",
   "metadata": {},
   "source": [
    "Empezamos aplicando el algoritmo `k-means`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354d4c5-7454-48b4-87c7-a6db5c44b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first, ellapsed = add_clustering_labels(\n",
    "    df_first,\n",
    "    compute_kmeans, \n",
    "    parameters = {\"number_clusters\": 4},\n",
    "    new_label_name = \"label_kmeans\"\n",
    ")\n",
    "\n",
    "# Añado el tiempo consumido\n",
    "ellapsed_times[\"label_kmeans\"] = ellapsed\n",
    "\n",
    "custom_pair_plot(df_first, hue = \"label_kmeans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf585a-e19c-4d7b-ba10-7337f3c44f70",
   "metadata": {},
   "source": [
    "Ahora aplicamos `dbscan` para la clusterización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed40ca5-056d-4a35-8eda-db33b4783b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first, ellapsed = add_clustering_labels(\n",
    "    df_first,\n",
    "    compute_dbscan,\n",
    "    parameters = {\"eps\": 0.4, \"min_samples\": 5},\n",
    "    new_label_name = \"label_dbscan\",\n",
    "    exclude_columns = [\"label_kmeans\"]\n",
    ") \n",
    "\n",
    "# Añado el tiempo consumido\n",
    "ellapsed_times[\"label_dbscan\"] = ellapsed\n",
    "\n",
    "custom_pair_plot(\n",
    "    df = df_first,\n",
    "    hue = \"label_dbscan\",\n",
    "    exclude_columns = [\"label_kmeans\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca63df-3019-4f18-93de-b4bd041fbb00",
   "metadata": {},
   "source": [
    "Ahora aplicamos `birch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f247b-a2e2-4cbd-a8d1-24fa6bac3ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -- probar con num_clusters = None que es lo que pone en la pagina de ejemplo\n",
    "# TODO -- quite eso porque tardaba demasiado\n",
    "df_first, ellapsed = add_clustering_labels(\n",
    "    df_first,\n",
    "    compute_birch,\n",
    "    parameters = {\"num_clusters\": 5, \"threshold\": 0.2},\n",
    "    new_label_name = \"label_birch\",\n",
    "    exclude_columns = [\"label_kmeans\", \"label_dbscan\"]\n",
    ") \n",
    "\n",
    "# Añado el tiempo consumido\n",
    "ellapsed_times[\"label_birch\"] = ellapsed\n",
    "\n",
    "custom_pair_plot(\n",
    "    df = df_first,\n",
    "    hue = \"label_birch\",\n",
    "    exclude_columns = [\"label_kmeans\", \"label_dbscan\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0f1a3-7ffa-4105-8599-d0f2da492ae8",
   "metadata": {},
   "source": [
    "Ahora aplicamos *Mean Shift*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a401b8-695e-421b-b14a-21b33827ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first, ellapsed = add_clustering_labels(\n",
    "    df_first,\n",
    "    compute_mean_shift,\n",
    "    parameters = {\"bandwidth\": None, \"max_iter\": 1_000},\n",
    "    new_label_name = \"label_mean_shift\",\n",
    "    exclude_columns = [\"label_kmeans\", \"label_dbscan\", \"label_birch\"]\n",
    ") \n",
    "\n",
    "# Añado el tiempo consumido\n",
    "ellapsed_times[\"label_mean_shift\"] = ellapsed\n",
    "\n",
    "custom_pair_plot(\n",
    "    df = df_first,\n",
    "    hue = \"label_mean_shift\",\n",
    "    exclude_columns = [\"label_kmeans\", \"label_dbscan\", \"label_birch\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406d60e-baf0-4668-adad-e53d08fc36d1",
   "metadata": {},
   "source": [
    "Ahora aplicamos *clustering* jerárquico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40c29f-8f2b-48d8-aa6c-883664bf2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first, ellapsed = add_clustering_labels(\n",
    "    df_first,\n",
    "    compute_agglomerative_clustering,\n",
    "    parameters = {\"n_clusters\": 5, \"linkage\": \"ward\"},\n",
    "    new_label_name = \"label_agglomerative\",\n",
    "    exclude_columns = [\"label_kmeans\", \"label_dbscan\", \"label_birch\", \"label_mean_shift\"]\n",
    ") \n",
    "\n",
    "# Añado el tiempo consumido\n",
    "ellapsed_times[\"label_agglomerative\"] = ellapsed\n",
    "\n",
    "custom_pair_plot(\n",
    "    df = df_first,\n",
    "    hue = \"label_agglomerative\",\n",
    "    exclude_columns = [\"label_kmeans\", \"label_dbscan\", \"label_birch\", \"label_mean_shift\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f0e4a-2554-49ee-8352-7ba9fd599026",
   "metadata": {},
   "source": [
    "## Métricas de las clusterizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d6f42-90c0-4b36-9ac9-afddb47b1192",
   "metadata": {},
   "source": [
    "Una vez que hemos aplicado los cinco algoritmos anteriores, recogemos las métricas de cada algoritmo para poder compararlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d40f17f-e1a5-454e-92fa-13d081fff3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las metricas que hemos construido\n",
    "labels = [\"label_kmeans\", \"label_dbscan\", \"label_birch\", \"label_mean_shift\", \"label_agglomerative\"]\n",
    "\n",
    "# Dataframe sin las columnas con las metricas\n",
    "df_raw = drop_columns(df_first, labels)\n",
    "\n",
    "# Diccionario que vamos a construir agregando todas las metricas\n",
    "global_metrics = dict()\n",
    "\n",
    "# Iteramos sobre cada una de las metricas\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Tomamos las etiquetas del algoritmo actual y calculamos las metricas+\n",
    "    curr_labels = df_first[label]\n",
    "    curr_metrics = compute_clustering_metrics(df_raw, curr_labels)\n",
    "    \n",
    "    # Añadimos el tiempo de ejecucion para que se muestre con el resto de metricas\n",
    "    curr_metrics[\"ellapsed_time (s)\"] = ellapsed_times[label]\n",
    "    \n",
    "    # Mostramos dichas metricas\n",
    "    pprint(curr_metrics)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Guardamos el diccionario con metricas en el nuevo diccionario con todas las metricas\n",
    "    global_metrics[label] = curr_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac288f1-59e7-4f1d-8888-7a2a5fb924f1",
   "metadata": {},
   "source": [
    "## Visualizando estas métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7824c-a8f5-4641-9e34-2330b584bbaa",
   "metadata": {},
   "source": [
    "Vamos a mostrar gráficamente algunas de estas métricas de forma comparativa entre los algoritmos considerados. Nos basamos fuertemente en el diccionario con las métricas agregadas de distintos algoritmos para realizar estas visualizaciones. Empezamos comparando los tiempos de ejecución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a9b20-82da-4d76-866f-1ca8f0bfc493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etiquetas de los algoritmos que hemos usado\n",
    "labels = [\"label_kmeans\", \"label_dbscan\", \"label_birch\", \"label_mean_shift\", \"label_agglomerative\"]\n",
    "\n",
    "# Tomamos los tiempos de ejecucion\n",
    "times = [global_metrics[label][\"ellapsed_time (s)\"] for label in labels]\n",
    "\n",
    "# Ordenamos las dos listas basandonos en la de tiempos, para obtener una mejor visualizacion\n",
    "times, labels = zip(*sorted(zip(times, labels)))\n",
    "\n",
    "# Creamos un dataframe para mostrar usando seaborn\n",
    "df_sns = pd.DataFrame()\n",
    "df_sns[\"labels\"] = labels\n",
    "df_sns[\"time (s)\"] = times\n",
    "\n",
    "# Mostramos la grafica buscada\n",
    "sns.barplot(data = df_sns, x = \"time (s)\", y = \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ba57f-12c5-4fa1-81c0-f5b8765e9a63",
   "metadata": {},
   "source": [
    "Veamos ahora el número de clusters que han generado los algoritmos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34314c97-4310-4ee0-abd9-bd340e59c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos el numero de clusters\n",
    "num_clusters = [global_metrics[label][\"total_clusters\"] for label in labels]\n",
    "\n",
    "# Ordenamos las dos listas basandonos en la de numero de clusters, para obtener una mejor visualizacion\n",
    "num_clusters, labels = zip(*sorted(zip(num_clusters, labels)))\n",
    "\n",
    "# Creamos un dataframe para mostrar usando seaborn\n",
    "df_sns = pd.DataFrame()\n",
    "df_sns[\"labels\"] = labels\n",
    "df_sns[\"total_clusters\"] = num_clusters\n",
    "\n",
    "# Mostramos la grafica buscada\n",
    "sns.barplot(data = df_sns, x = \"total_clusters\", y = \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889915d1-774d-4d47-8143-a9b0fad81e47",
   "metadata": {},
   "source": [
    "Comparamos ahora los índices de Silhouette, que recordemos que queremos que sean lo más cercano a 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fb0a8-fdf9-426b-b72f-fba780f0bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = [global_metrics[label][\"silhouette\"] for label in labels]\n",
    "silhouette, labels = zip(*sorted(zip(silhouette, labels)))\n",
    "\n",
    "# Creamos un dataframe para mostrar usando seaborn\n",
    "df_sns = pd.DataFrame()\n",
    "df_sns[\"labels\"] = labels\n",
    "df_sns[\"silhouette\"] = silhouette\n",
    "\n",
    "# Mostramos la grafica buscada\n",
    "sns.barplot(data = df_sns, x = \"silhouette\", y = \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008227e6-5886-457c-894d-c33c5ba6eeac",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con Calinski:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1bd841-8526-463e-adbd-3b40c425bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "calinski = [global_metrics[label][\"calinski\"] for label in labels]\n",
    "calinski, labels = zip(*sorted(zip(calinski, labels)))\n",
    "\n",
    "# Creamos un dataframe para mostrar usando seaborn\n",
    "df_sns = pd.DataFrame()\n",
    "df_sns[\"labels\"] = labels\n",
    "df_sns[\"calinski\"] = calinski\n",
    "\n",
    "# Mostramos la grafica buscada\n",
    "sns.barplot(data = df_sns, x = \"calinski\", y = \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8359b3f-18c8-465d-937c-0ccedb522a58",
   "metadata": {},
   "source": [
    "Hacemos lo mismo con Davies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca42509-ad55-42ee-82e6-981e6bbe5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "davies = [global_metrics[label][\"davies\"] for label in labels]\n",
    "davies, labels = zip(*sorted(zip(davies, labels)))\n",
    "\n",
    "# Creamos un dataframe para mostrar usando seaborn\n",
    "df_sns = pd.DataFrame()\n",
    "df_sns[\"labels\"] = labels\n",
    "df_sns[\"davies\"] = davies\n",
    "\n",
    "# Mostramos la grafica buscada\n",
    "sns.barplot(data = df_sns, x = \"davies\", y = \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab9d94-ba06-47e5-afc1-c4b5efa4c8fa",
   "metadata": {},
   "source": [
    "Y veamos ahora las dos estadísticas sobre el número de elementos por cada cluster:\n",
    "\n",
    "<!-- TODO -- juntar estas dos graficas en una sola -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a954dd-8165-452b-b87e-6714ac1b26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_mean = [global_metrics[label][\"elements_mean\"] for label in labels]\n",
    "elements_mean, labels = zip(*sorted(zip(elements_mean, labels)))\n",
    "\n",
    "# Creamos un dataframe para mostrar usando seaborn\n",
    "df_sns = pd.DataFrame()\n",
    "df_sns[\"labels\"] = labels\n",
    "df_sns[\"elements_mean\"] = elements_mean\n",
    "\n",
    "# Mostramos la grafica buscada\n",
    "sns.barplot(data = df_sns, x = \"elements_mean\", y = \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92ffa6-31fb-44ec-adc5-326678070199",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_std = [global_metrics[label][\"elements_std\"] for label in labels]\n",
    "elements_std, labels = zip(*sorted(zip(elements_std, labels)))\n",
    "\n",
    "# Creamos un dataframe para mostrar usando seaborn\n",
    "df_sns = pd.DataFrame()\n",
    "df_sns[\"labels\"] = labels\n",
    "df_sns[\"elements_std\"] = elements_std\n",
    "\n",
    "# Mostramos la grafica buscada\n",
    "sns.barplot(data = df_sns, x = \"elements_std\", y = \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7ebec-a99e-44f0-8762-05a072d59b1e",
   "metadata": {},
   "source": [
    "## Visualizando en 2D la clusterización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c8536e-a308-4165-b117-cba02f3fef91",
   "metadata": {},
   "source": [
    "Usamos una técnica como `PCA` para reducir el conjunto de datos multidimensional a tener tan solo 2 dimensiones. A partir de aquí, mostramos gráficamente la clusterización que hemos realizado con cada uno de los algoritmos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbc0a4-7610-441d-a007-368efa8f4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducimos la dimensionalidad del conjunto de datos\n",
    "pca = PCA(2)\n",
    "\n",
    "# Reducimos la dimensionalidad\n",
    "df_lowdimension = pd.DataFrame(pca.fit_transform(df_raw))\n",
    "\n",
    "# Renombramos las columnas\n",
    "df_lowdimension = df_lowdimension.rename(columns = {\n",
    "    0: \"first_axis\",\n",
    "    1: \"second_axis\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c65e5-71b2-434c-a9f9-2edc0493e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por cada algoritmo, mostramos la clusterizacion realizada\n",
    "for label in labels:\n",
    "    print(f\"Grafico usando las etiquetas de {label}\")\n",
    "    sns.scatterplot(data = df_lowdimension, x = \"first_axis\", y = \"second_axis\", hue = df_first[label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a93c82-595f-4452-b6ca-624c71cd2231",
   "metadata": {},
   "source": [
    "Realizamos lo mismo pero usando `TSNE` como algoritmo para reducir la dimensionalidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45dd26-c646-4f45-b9f0-4c81bf8615a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducimos la dimensionalidad\n",
    "df_lowdimension = pd.DataFrame(TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(df_raw))\n",
    "\n",
    "# Renombramos las columnas\n",
    "df_lowdimension = df_lowdimension.rename(columns = {\n",
    "    0: \"first_axis\",\n",
    "    1: \"second_axis\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860152fc-dbe7-455b-b131-e2d6a708f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por cada algoritmo, mostramos la clusterizacion realizada\n",
    "for label in labels:\n",
    "    print(f\"Grafico usando las etiquetas de {label}\")\n",
    "    sns.scatterplot(data = df_lowdimension, x = \"first_axis\", y = \"second_axis\", hue = df_first[label])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
